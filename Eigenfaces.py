# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1fuKOHr1x-pDwZ2E-OMveWGG-boHqljxr
"""

import numpy as np
import pandas as pd
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
from sklearn.model_selection import train_test_split
from time import time
import matplotlib.pyplot as plt
from scipy.stats import loguniform
from sklearn.datasets import fetch_lfw_people
from sklearn.decomposition import PCA
from sklearn.metrics import ConfusionMatrixDisplay, classification_report
from sklearn.model_selection import RandomizedSearchCV, train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, confusion_matrix

"""Task 1       
Data Pre Processing
"""

lfw_people = fetch_lfw_people(min_faces_per_person=70, resize=0.4)

# introspect the images arrays to find the shapes (for plotting)
n_samples, h, w = lfw_people.images.shape

# for machine learning we use the 2 data directly (as relative pixel
# positions info is ignored by this model)
X = lfw_people.data
n_features = X.shape[1]

# the label to predict is the id of the person
y = lfw_people.target
target_names = lfw_people.target_names
n_classes = target_names.shape[0]

print("Total dataset size:")
print("n_samples: %d" % n_samples)
print("n_features: %d" % n_features)
print("n_classes: %d" % n_classes)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

print("number of faces:",X_train.shape[0])

"""Task 2     
Eigenface Implementation
"""

pca = PCA().fit(X_train)

# Plot the explained variance ratio
plt.figure(figsize=(10, 6))
plt.plot(np.cumsum(pca.explained_variance_ratio_))
plt.xlabel('Number of Components')
plt.ylabel('Cumulative Explained Variance')
plt.title('Explained Variance vs. Number of Components')
plt.grid(True)
plt.show()

# Choose an appropriate value for n_components based on the plot
cumulative_variance_ratio = np.cumsum(pca.explained_variance_ratio_)
threshold = 0.95 
n_components = np.argmax(cumulative_variance_ratio >= threshold) + 1

print("n_components",n_components)
print("\n")

# Apply PCA with the chosen n_components to training and testing sets
pca = PCA(n_components=n_components).fit(X_train)
X_train_pca = pca.transform(X_train)
X_test_pca = pca.transform(X_test)

"""Task 3    
Model Training
"""

# Train a k-Nearest Neighbors (KNN) classifier on the reduced feature space
knn_classifier = KNeighborsClassifier(n_neighbors=5)
knn_classifier.fit(X_train_pca, y_train)

# Predict names on the test set
y_pred = knn_classifier.predict(X_test_pca)

# Reconstruct the data using the selected components for visualization
X_test_reconstructed = pca.inverse_transform(X_test_pca)
X_test_reconstructed_images = X_test_reconstructed.reshape((X_test.shape[0], h, w))

# Plot original and reconstructed faces with actual and predicted names
fig, axes = plt.subplots(2, 10, figsize=(20, 5),
                         subplot_kw={'xticks': [], 'yticks': []},
                         gridspec_kw=dict(hspace=0.3, wspace=0.1))

for i in range(10):
    axes[0, i].imshow(X_test[i].reshape((h, w)), cmap='gray')
    axes[0, i].set_title(f"Actual:\n{target_names[y_test[i]]}", fontsize=8, wrap=True)

    axes[1, i].imshow(X_test_reconstructed_images[i], cmap='gray')
    axes[1, i].set_title(f"Predicted:\n{target_names[y_pred[i]]}", fontsize=8, wrap=True)

plt.show()

"""Task 4   
Model Evaluation
"""

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy on the test set: {accuracy:.2f}")

# Visualize a subset of Eigenfaces
num_eigenfaces_to_display = 10
eigenfaces_subset = pca.components_[:num_eigenfaces_to_display]

fig, axes = plt.subplots(1, num_eigenfaces_to_display, figsize=(15, 3),
                         subplot_kw={'xticks': [], 'yticks': []},
                         gridspec_kw=dict(wspace=0.1, hspace=0.1))

for i, eigenface in enumerate(eigenfaces_subset):
    axes[i].imshow(eigenface.reshape((h, w)), cmap='gray')
    axes[i].set_title(f'Eigenface {i + 1}')

plt.show()

# Report: Observations on model failures and ways to improve
conf_matrix = confusion_matrix(y_test, y_pred)
print("Confusion Matrix:")
print(conf_matrix)

# Analyze misclassifications
misclassified_indices = np.where(y_test != y_pred)[0]

print("\nMisclassified Images:")
for misclassified_index in misclassified_indices[:min(5, len(misclassified_indices))]:
    actual_name = target_names[y_test[misclassified_index]]
    predicted_name = target_names[y_pred[misclassified_index]]
    print(f"Actual: {actual_name}, Predicted: {predicted_name}")

"""Task 5   
Experimenting with different values of n_components
"""

n_components_values = [10, 20, 30, 40, 50, 60, 70, 80, 90, 100]

for n_components in n_components_values:
    # Apply PCA to training and testing sets
    pca = PCA(n_components=n_components).fit(X_train)
    X_train_pca = pca.transform(X_train)
    X_test_pca = pca.transform(X_test)

    # Train a k-Nearest Neighbors (KNN) classifier on the reduced feature space
    knn_classifier = KNeighborsClassifier(n_neighbors=5)
    knn_classifier.fit(X_train_pca, y_train)

    # Predict names on the test set
    y_pred = knn_classifier.predict(X_test_pca)

    # Calculate accuracy
    accuracy = accuracy_score(y_test, y_pred)
    print(f"Accuracy with {n_components} components: {accuracy:.2f}")